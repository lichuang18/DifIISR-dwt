# DifIISR-DWT 方案A配置 - 纯PyTorch版本，支持LPIPS梯度回传
#
# 与 DWTModelAllBandsLearnable 的区别:
#   - DWTModelAllBandsLearnable: decode使用pywt (numpy)，梯度断开，LPIPS无效
#   - DWTModelAllBandsLearnableTorch: decode使用纯PyTorch，梯度可回传，LPIPS有效
#
# 预期效果:
#   - LPIPS损失可以有效训练 self.up 和 self.down 参数
#   - 视觉质量更好（感知损失有效）

exp_name: DifIISR_DWT_Learnable_Torch

model:
  target: models.unet.UNetModelSwin
  ckpt_path: ~  # 从头训练
  params:
    image_size: 64
    in_channels: 42     # 21(噪声子带) + 21(LQ条件)
    model_channels: 160
    out_channels: 21    # 输出全部子带 (level2: 12 + level1: 9)
    cond_lq: True
    attention_resolutions: [64,32,16,8]
    dropout: 0
    channel_mult: [1, 2, 2, 4]
    num_res_blocks: [2, 2, 2, 2]
    conv_resample: True
    dims: 2
    use_fp16: False
    num_head_channels: 32
    use_scale_shift_norm: True
    resblock_updown: False
    swin_depth: 2
    swin_embed_dim: 192
    window_size: 8
    mlp_ratio: 4

diffusion:
  target: models.script_util.create_gaussian_diffusion_test
  params:
    sf: 4
    schedule_name: exponential
    schedule_kwargs:
      power: 0.3
    etas_end: 0.99
    steps: 15
    min_noise_level: 0.04
    kappa: 2.0
    weighted_mse: False
    predict_type: xstart
    timestep_respacing: ~
    scale_factor: 1.0
    normalize_input: True
    latent_flag: True

# 使用纯PyTorch版本，支持LPIPS梯度回传
autoencoder:
  target: ldm.models.dwt_autoencoder.DWTModelAllBandsLearnableTorch
  ckpt_path: ~
  use_fp16: False
  params:
    wavelet: haar
    level: 2

# 感知损失配置 - 渐进式权重调度 + 时间步动态权重
#
# 渐进式权重调度策略:
#   - 训练初期: lpips_weight_start=0.01, LPIPS贡献小, MSE主导
#   - 训练后期: lpips_weight_end=0.15, LPIPS与MSE接近等权
#   - 线性插值: weight = start + (end - start) * (iter / total)
#
# 时间步动态权重 (在train.py中实现):
#   t < T/3:  权重 1.0 (pred质量好)
#   T/3 <= t < 2T/3: 权重 0.1
#   t >= 2T/3: 权重 0.001 (pred质量差)
#
# 预期效果 (total=100K, 典型LPIPS≈0.03加权后):
#   0K:   weight=0.01, LPIPS贡献≈0.0003, MSE≈0.01  → 比例1:33 (MSE主导)
#   50K:  weight=0.08, LPIPS贡献≈0.0024, MSE≈0.005 → 比例1:2
#   100K: weight=0.15, LPIPS贡献≈0.0045, MSE≈0.005 → 比例1:1 (接近等权)
perceptual_loss:
  enabled: True
  lpips_weight_start: 0.01   # 起始权重 (训练初期)
  lpips_weight_end: 0.15     # 结束权重 (训练后期)
  lpips_net: 'vgg'

# Real-ESRGAN风格退化配置
degradation:
  sf: 4

  # 第一阶段退化
  blur_kernel_size: 21
  kernel_list: ['iso', 'aniso', 'generalized_iso', 'generalized_aniso', 'plateau_iso', 'plateau_aniso']
  kernel_prob: [0.45, 0.25, 0.12, 0.03, 0.12, 0.03]
  blur_sigma: [0.2, 3.0]
  betag_range: [0.5, 4.0]
  betap_range: [1, 2.0]

  resize_prob: [0.2, 0.7, 0.1]
  resize_range: [0.15, 1.5]

  gaussian_noise_prob: 0.5
  noise_range: [1, 30]
  poisson_scale_range: [0.05, 3.0]
  gray_noise_prob: 0.4

  jpeg_range: [30, 95]

  # 第二阶段退化
  second_order_prob: 0.5
  second_blur_prob: 0.8

  blur_kernel_size2: 15
  kernel_list2: ['iso', 'aniso', 'generalized_iso', 'generalized_aniso', 'plateau_iso', 'plateau_aniso']
  kernel_prob2: [0.45, 0.25, 0.12, 0.03, 0.12, 0.03]
  blur_sigma2: [0.2, 1.5]
  betag_range2: [0.5, 4.0]
  betap_range2: [1, 2.0]

  resize_prob2: [0.3, 0.4, 0.3]
  resize_range2: [0.3, 1.2]

  gaussian_noise_prob2: 0.5
  noise_range2: [1, 25]
  poisson_scale_range2: [0.05, 2.5]
  gray_noise_prob2: 0.4

  jpeg_range2: [30, 95]

data:
  type: degradation
  train:
    hr_dir: /home/lch/sr_recons/dataset/M3FD_processed/train/HR
    gt_size: 256
    scale: 4
    use_hflip: True
    use_rot: True
    length: 2000 #~
  val:
    hr_dir: /home/lch/sr_recons/dataset/M3FD_processed/val/HR
    length: 20

train:
  output_dir: /home/lch/sr_recons/experiments
  batch_size: 8
  val_batch_size: 4
  lr: 5e-5
  weight_decay: 0.01
  iterations: 5000
  min_lr: 1e-6

  use_ema: True
  ema_rate: 0.999
  use_scheduler: True

  log_freq: 100
  val_freq: 1000
  save_freq: 1000

  num_workers: 4
